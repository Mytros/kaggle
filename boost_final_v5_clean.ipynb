{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d78f0f0-fb69-49b4-9f2c-b919760faa7f",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "9d78f0f0-fb69-49b4-9f2c-b919760faa7f",
        "outputId": "209366dd-c73b-4886-fd68-45476e016818"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows: 15,000 | Features: 10 | Num: 8 | Cat: 2\n",
            "\n",
            "Training (ES, CV)...\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000342 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 1:  XGB=0.80078  LGBM=0.77783  ET=0.78906\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000369 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 2:  XGB=0.90820  LGBM=0.89746  ET=0.91211\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000333 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 3:  XGB=0.93555  LGBM=0.93164  ET=0.92676\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000530 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 4:  XGB=0.91992  LGBM=0.93555  ET=0.92676\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000430 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 5:  XGB=0.98828  LGBM=0.98633  ET=0.97363\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000457 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 6:  XGB=0.92383  LGBM=0.91406  ET=0.90039\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000339 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 7:  XGB=0.90820  LGBM=0.90918  ET=0.88672\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000427 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 8:  XGB=0.94824  LGBM=0.93262  ET=0.92578\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000346 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 9:  XGB=0.94824  LGBM=0.93750  ET=0.94141\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000381 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 10:  XGB=0.92676  LGBM=0.92871  ET=0.91406\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000401 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 840\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 11:  XGB=0.93066  LGBM=0.90527  ET=0.94238\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000371 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 12:  XGB=0.96484  LGBM=0.95508  ET=0.96484\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000365 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 13:  XGB=0.95410  LGBM=0.94531  ET=0.94531\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000399 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 14:  XGB=0.97949  LGBM=0.97852  ET=0.97656\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000345 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 15:  XGB=0.89160  LGBM=0.88965  ET=0.88965\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000497 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 16:  XGB=0.95703  LGBM=0.95801  ET=0.94336\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000383 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 17:  XGB=0.91309  LGBM=0.89355  ET=0.90430\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000365 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 18:  XGB=0.96289  LGBM=0.95898  ET=0.94043\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000466 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 19:  XGB=0.96582  LGBM=0.95898  ET=0.95117\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000351 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 20:  XGB=0.95312  LGBM=0.95117  ET=0.94238\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000399 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 21:  XGB=0.95898  LGBM=0.96387  ET=0.94824\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000401 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 22:  XGB=0.88281  LGBM=0.86816  ET=0.86035\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000327 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 23:  XGB=0.82520  LGBM=0.80957  ET=0.80371\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000901 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 24:  XGB=0.96680  LGBM=0.95996  ET=0.95312\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000338 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 25:  XGB=0.96973  LGBM=0.95801  ET=0.95508\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000368 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 26:  XGB=0.93555  LGBM=0.92969  ET=0.92578\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000401 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 27:  XGB=0.92480  LGBM=0.92773  ET=0.90723\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000485 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 28:  XGB=0.95605  LGBM=0.95020  ET=0.94336\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000411 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 29:  XGB=0.87402  LGBM=0.85547  ET=0.86328\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000347 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 30:  XGB=0.93457  LGBM=0.92773  ET=0.90723\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000451 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 31:  XGB=0.93359  LGBM=0.93164  ET=0.92871\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000371 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 32:  XGB=0.93066  LGBM=0.92285  ET=0.92188\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000440 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 33:  XGB=0.95801  LGBM=0.96387  ET=0.94629\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000528 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 34:  XGB=0.95508  LGBM=0.94727  ET=0.93164\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000478 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 35:  XGB=0.96289  LGBM=0.94434  ET=0.94336\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000365 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 36:  XGB=0.94727  LGBM=0.93652  ET=0.93652\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000297 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 37:  XGB=0.92139  LGBM=0.91602  ET=0.90234\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000420 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 38:  XGB=0.92871  LGBM=0.92090  ET=0.91797\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000449 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 39:  XGB=0.91113  LGBM=0.87793  ET=0.87305\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000334 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 40:  XGB=0.95703  LGBM=0.94727  ET=0.95117\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000403 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 41:  XGB=0.99316  LGBM=0.98828  ET=0.98730\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000469 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 42:  XGB=0.96387  LGBM=0.95801  ET=0.95020\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000404 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 43:  XGB=0.90820  LGBM=0.90137  ET=0.90527\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000319 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 44:  XGB=0.93848  LGBM=0.91406  ET=0.90918\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000332 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 45:  XGB=0.95508  LGBM=0.94043  ET=0.92480\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000378 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 46:  XGB=0.89844  LGBM=0.87500  ET=0.87305\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000361 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 47:  XGB=0.93652  LGBM=0.92090  ET=0.91309\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000380 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 48:  XGB=0.92334  LGBM=0.89844  ET=0.91504\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000360 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 49:  XGB=0.87500  LGBM=0.88672  ET=0.87793\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000540 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 50:  XGB=0.97949  LGBM=0.97266  ET=0.97070\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000344 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 51:  XGB=0.94775  LGBM=0.93555  ET=0.93750\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000378 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 52:  XGB=0.91309  LGBM=0.90723  ET=0.90820\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000397 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 53:  XGB=0.99121  LGBM=0.96777  ET=0.96777\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000433 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 54:  XGB=0.97266  LGBM=0.97461  ET=0.95605\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000336 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 55:  XGB=0.96094  LGBM=0.95508  ET=0.95215\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000383 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 56:  XGB=0.89160  LGBM=0.87793  ET=0.87695\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000399 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 57:  XGB=0.93164  LGBM=0.93066  ET=0.93262\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000507 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 58:  XGB=0.92676  LGBM=0.91602  ET=0.91309\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000355 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 59:  XGB=0.94043  LGBM=0.91797  ET=0.93457\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000379 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 60:  XGB=0.96875  LGBM=0.96680  ET=0.95801\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000478 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 61:  XGB=0.98242  LGBM=0.96484  ET=0.96484\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000352 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 62:  XGB=0.89258  LGBM=0.87598  ET=0.86621\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000344 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 63:  XGB=0.97168  LGBM=0.96680  ET=0.95508\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000434 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 64:  XGB=0.97949  LGBM=0.98340  ET=0.98828\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000370 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 65:  XGB=0.93164  LGBM=0.92969  ET=0.90527\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000444 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 66:  XGB=0.92676  LGBM=0.90967  ET=0.88574\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000863 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 67:  XGB=0.95508  LGBM=0.94922  ET=0.93848\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000382 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 68:  XGB=0.96191  LGBM=0.95410  ET=0.96191\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000363 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 69:  XGB=0.96582  LGBM=0.95996  ET=0.96094\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000413 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 70:  XGB=0.94678  LGBM=0.92188  ET=0.93652\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000499 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 71:  XGB=0.97656  LGBM=0.97656  ET=0.96875\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000365 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 72:  XGB=0.95312  LGBM=0.95215  ET=0.95312\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000495 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 73:  XGB=0.92871  LGBM=0.92383  ET=0.90723\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000354 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 74:  XGB=0.87891  LGBM=0.87012  ET=0.86133\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000351 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 75:  XGB=0.93359  LGBM=0.91992  ET=0.89355\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000408 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 76:  XGB=0.96973  LGBM=0.96484  ET=0.93555\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000390 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 77:  XGB=0.95508  LGBM=0.93555  ET=0.94434\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000483 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 78:  XGB=0.90527  LGBM=0.90088  ET=0.88379\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000399 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 79:  XGB=0.95801  LGBM=0.93750  ET=0.95703\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000346 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 80:  XGB=0.97070  LGBM=0.95020  ET=0.96387\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000366 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 81:  XGB=0.97949  LGBM=0.97559  ET=0.97656\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000508 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 82:  XGB=0.96777  LGBM=0.96484  ET=0.96289\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000364 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 83:  XGB=0.95801  LGBM=0.95605  ET=0.94824\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000362 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 840\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 84:  XGB=0.93750  LGBM=0.93066  ET=0.93652\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000462 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 85:  XGB=0.98145  LGBM=0.97754  ET=0.97363\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000371 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 86:  XGB=0.96387  LGBM=0.95703  ET=0.94434\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000411 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 87:  XGB=0.99316  LGBM=0.98730  ET=0.98633\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000428 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 88:  XGB=0.92383  LGBM=0.92188  ET=0.90430\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000371 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 89:  XGB=0.95264  LGBM=0.94824  ET=0.92578\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000471 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 90:  XGB=0.99316  LGBM=0.99023  ET=0.98633\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000417 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 91:  XGB=0.89600  LGBM=0.87061  ET=0.88672\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000436 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 92:  XGB=0.93848  LGBM=0.92969  ET=0.93262\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000367 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 93:  XGB=0.98682  LGBM=0.98145  ET=0.96875\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000443 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 94:  XGB=0.98047  LGBM=0.97070  ET=0.96191\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000639 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 95:  XGB=0.98047  LGBM=0.97559  ET=0.97852\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000430 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 96:  XGB=0.96973  LGBM=0.95996  ET=0.97363\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000421 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 97:  XGB=0.91504  LGBM=0.90234  ET=0.89551\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000453 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 98:  XGB=0.97754  LGBM=0.97852  ET=0.95410\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000465 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 99:  XGB=0.94531  LGBM=0.93066  ET=0.91602\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000539 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 100:  XGB=0.95410  LGBM=0.94043  ET=0.92578\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000448 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 101:  XGB=0.84473  LGBM=0.82812  ET=0.82910\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000412 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 102:  XGB=0.88672  LGBM=0.85693  ET=0.84180\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000421 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 103:  XGB=0.91211  LGBM=0.91797  ET=0.88379\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000466 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 104:  XGB=1.00000  LGBM=1.00000  ET=1.00000\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000480 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 105:  XGB=0.87695  LGBM=0.89648  ET=0.88281\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000408 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 106:  XGB=0.95312  LGBM=0.94922  ET=0.92578\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000375 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 107:  XGB=0.93848  LGBM=0.94043  ET=0.93945\n",
            "[LightGBM] [Info] Number of positive: 2426, number of negative: 9494\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000387 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203523 -> initscore=-1.364416\n",
            "[LightGBM] [Info] Start training from score -1.364416\n",
            "Fold 108:  XGB=0.96582  LGBM=0.95703  ET=0.95020\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000342 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 109:  XGB=0.91410  LGBM=0.89542  ET=0.90196\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000394 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 110:  XGB=0.99533  LGBM=0.98599  ET=0.98880\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000420 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 111:  XGB=0.95612  LGBM=0.95051  ET=0.92997\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000626 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 112:  XGB=0.97852  LGBM=0.97666  ET=0.97012\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000412 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 113:  XGB=0.94958  LGBM=0.95331  ET=0.94304\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000398 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 114:  XGB=0.93557  LGBM=0.93838  ET=0.90943\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000399 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 115:  XGB=0.91223  LGBM=0.91036  ET=0.88422\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000434 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 116:  XGB=0.93464  LGBM=0.92437  ET=0.88982\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000466 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 117:  XGB=0.96265  LGBM=0.96919  ET=0.95518\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000383 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 118:  XGB=0.95425  LGBM=0.93838  ET=0.94024\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000399 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 119:  XGB=0.94444  LGBM=0.93371  ET=0.94118\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000467 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 120:  XGB=0.96172  LGBM=0.95985  ET=0.96545\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000421 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 121:  XGB=0.97479  LGBM=0.97106  ET=0.95892\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000486 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 122:  XGB=0.97479  LGBM=0.96639  ET=0.96545\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000394 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 123:  XGB=0.89916  LGBM=0.89356  ET=0.86648\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000400 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 124:  XGB=0.97199  LGBM=0.96732  ET=0.97386\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000542 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 125:  XGB=0.97012  LGBM=0.95145  ET=0.95705\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000783 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 126:  XGB=0.95098  LGBM=0.93651  ET=0.92717\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000363 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 127:  XGB=0.89729  LGBM=0.87115  ET=0.87862\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000375 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 128:  XGB=0.94771  LGBM=0.92997  ET=0.93277\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000441 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 129:  XGB=0.94678  LGBM=0.93557  ET=0.92997\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000524 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 130:  XGB=0.93277  LGBM=0.93184  ET=0.90756\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000349 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 131:  XGB=0.89169  LGBM=0.87395  ET=0.86181\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000375 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 843\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 132:  XGB=0.93978  LGBM=0.92063  ET=0.91970\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000367 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 133:  XGB=0.96265  LGBM=0.95798  ET=0.96078\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000474 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 134:  XGB=0.94444  LGBM=0.94865  ET=0.93091\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000514 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 135:  XGB=0.83847  LGBM=0.82820  ET=0.83193\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000414 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 136:  XGB=0.98413  LGBM=0.98226  ET=0.97292\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000474 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 137:  XGB=0.95425  LGBM=0.94678  ET=0.94211\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000603 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 138:  XGB=0.96452  LGBM=0.95331  ET=0.95425\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000334 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 139:  XGB=0.92717  LGBM=0.92157  ET=0.91130\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000695 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 140:  XGB=0.88609  LGBM=0.88049  ET=0.85994\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000354 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 141:  XGB=0.97059  LGBM=0.95051  ET=0.94398\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000471 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 142:  XGB=0.94865  LGBM=0.95238  ET=0.94211\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000367 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 143:  XGB=0.98039  LGBM=0.97572  ET=0.96359\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000345 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 144:  XGB=0.95238  LGBM=0.95238  ET=0.94024\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000395 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 145:  XGB=0.90289  LGBM=0.88889  ET=0.88889\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000414 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 146:  XGB=0.96732  LGBM=0.95892  ET=0.95238\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000469 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 147:  XGB=0.97199  LGBM=0.95798  ET=0.95985\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000425 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 842\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 148:  XGB=0.95798  LGBM=0.94958  ET=0.94118\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000410 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 149:  XGB=0.92997  LGBM=0.92904  ET=0.90476\n",
            "[LightGBM] [Info] Number of positive: 2425, number of negative: 9495\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000532 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 11920, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203440 -> initscore=-1.364934\n",
            "[LightGBM] [Info] Start training from score -1.364934\n",
            "Fold 150:  XGB=0.97852  LGBM=0.96265  ET=0.96919\n",
            "\n",
            "XGB  OOF AUC: 0.90439 | Folds: [0.80078, 0.9082, 0.93555, 0.91992, 0.98828, 0.92383, 0.9082, 0.94824, 0.94824, 0.92676, 0.93066, 0.96484, 0.9541, 0.97949, 0.8916, 0.95703, 0.91309, 0.96289, 0.96582, 0.95312, 0.95898, 0.88281, 0.8252, 0.9668, 0.96973, 0.93555, 0.9248, 0.95605, 0.87402, 0.93457, 0.93359, 0.93066, 0.95801, 0.95508, 0.96289, 0.94727, 0.92139, 0.92871, 0.91113, 0.95703, 0.99316, 0.96387, 0.9082, 0.93848, 0.95508, 0.89844, 0.93652, 0.92334, 0.875, 0.97949, 0.94775, 0.91309, 0.99121, 0.97266, 0.96094, 0.8916, 0.93164, 0.92676, 0.94043, 0.96875, 0.98242, 0.89258, 0.97168, 0.97949, 0.93164, 0.92676, 0.95508, 0.96191, 0.96582, 0.94678, 0.97656, 0.95312, 0.92871, 0.87891, 0.93359, 0.96973, 0.95508, 0.90527, 0.95801, 0.9707, 0.97949, 0.96777, 0.95801, 0.9375, 0.98145, 0.96387, 0.99316, 0.92383, 0.95264, 0.99316, 0.896, 0.93848, 0.98682, 0.98047, 0.98047, 0.96973, 0.91504, 0.97754, 0.94531, 0.9541, 0.84473, 0.88672, 0.91211, 1.0, 0.87695, 0.95312, 0.93848, 0.96582, 0.9141, 0.99533, 0.95612, 0.97852, 0.94958, 0.93557, 0.91223, 0.93464, 0.96265, 0.95425, 0.94444, 0.96172, 0.97479, 0.97479, 0.89916, 0.97199, 0.97012, 0.95098, 0.89729, 0.94771, 0.94678, 0.93277, 0.89169, 0.93978, 0.96265, 0.94444, 0.83847, 0.98413, 0.95425, 0.96452, 0.92717, 0.88609, 0.97059, 0.94865, 0.98039, 0.95238, 0.90289, 0.96732, 0.97199, 0.95798, 0.92997, 0.97852]\n",
            "LGBM OOF AUC: 0.93678 | Folds: [0.77783, 0.89746, 0.93164, 0.93555, 0.98633, 0.91406, 0.90918, 0.93262, 0.9375, 0.92871, 0.90527, 0.95508, 0.94531, 0.97852, 0.88965, 0.95801, 0.89355, 0.95898, 0.95898, 0.95117, 0.96387, 0.86816, 0.80957, 0.95996, 0.95801, 0.92969, 0.92773, 0.9502, 0.85547, 0.92773, 0.93164, 0.92285, 0.96387, 0.94727, 0.94434, 0.93652, 0.91602, 0.9209, 0.87793, 0.94727, 0.98828, 0.95801, 0.90137, 0.91406, 0.94043, 0.875, 0.9209, 0.89844, 0.88672, 0.97266, 0.93555, 0.90723, 0.96777, 0.97461, 0.95508, 0.87793, 0.93066, 0.91602, 0.91797, 0.9668, 0.96484, 0.87598, 0.9668, 0.9834, 0.92969, 0.90967, 0.94922, 0.9541, 0.95996, 0.92188, 0.97656, 0.95215, 0.92383, 0.87012, 0.91992, 0.96484, 0.93555, 0.90088, 0.9375, 0.9502, 0.97559, 0.96484, 0.95605, 0.93066, 0.97754, 0.95703, 0.9873, 0.92188, 0.94824, 0.99023, 0.87061, 0.92969, 0.98145, 0.9707, 0.97559, 0.95996, 0.90234, 0.97852, 0.93066, 0.94043, 0.82812, 0.85693, 0.91797, 1.0, 0.89648, 0.94922, 0.94043, 0.95703, 0.89542, 0.98599, 0.95051, 0.97666, 0.95331, 0.93838, 0.91036, 0.92437, 0.96919, 0.93838, 0.93371, 0.95985, 0.97106, 0.96639, 0.89356, 0.96732, 0.95145, 0.93651, 0.87115, 0.92997, 0.93557, 0.93184, 0.87395, 0.92063, 0.95798, 0.94865, 0.8282, 0.98226, 0.94678, 0.95331, 0.92157, 0.88049, 0.95051, 0.95238, 0.97572, 0.95238, 0.88889, 0.95892, 0.95798, 0.94958, 0.92904, 0.96265]\n",
            "ET   OOF AUC:  0.92753 | Folds: [0.78906, 0.91211, 0.92676, 0.92676, 0.97363, 0.90039, 0.88672, 0.92578, 0.94141, 0.91406, 0.94238, 0.96484, 0.94531, 0.97656, 0.88965, 0.94336, 0.9043, 0.94043, 0.95117, 0.94238, 0.94824, 0.86035, 0.80371, 0.95312, 0.95508, 0.92578, 0.90723, 0.94336, 0.86328, 0.90723, 0.92871, 0.92188, 0.94629, 0.93164, 0.94336, 0.93652, 0.90234, 0.91797, 0.87305, 0.95117, 0.9873, 0.9502, 0.90527, 0.90918, 0.9248, 0.87305, 0.91309, 0.91504, 0.87793, 0.9707, 0.9375, 0.9082, 0.96777, 0.95605, 0.95215, 0.87695, 0.93262, 0.91309, 0.93457, 0.95801, 0.96484, 0.86621, 0.95508, 0.98828, 0.90527, 0.88574, 0.93848, 0.96191, 0.96094, 0.93652, 0.96875, 0.95312, 0.90723, 0.86133, 0.89355, 0.93555, 0.94434, 0.88379, 0.95703, 0.96387, 0.97656, 0.96289, 0.94824, 0.93652, 0.97363, 0.94434, 0.98633, 0.9043, 0.92578, 0.98633, 0.88672, 0.93262, 0.96875, 0.96191, 0.97852, 0.97363, 0.89551, 0.9541, 0.91602, 0.92578, 0.8291, 0.8418, 0.88379, 1.0, 0.88281, 0.92578, 0.93945, 0.9502, 0.90196, 0.9888, 0.92997, 0.97012, 0.94304, 0.90943, 0.88422, 0.88982, 0.95518, 0.94024, 0.94118, 0.96545, 0.95892, 0.96545, 0.86648, 0.97386, 0.95705, 0.92717, 0.87862, 0.93277, 0.92997, 0.90756, 0.86181, 0.9197, 0.96078, 0.93091, 0.83193, 0.97292, 0.94211, 0.95425, 0.9113, 0.85994, 0.94398, 0.94211, 0.96359, 0.94024, 0.88889, 0.95238, 0.95985, 0.94118, 0.90476, 0.96919]\n",
            "\n",
            "Best OOF blend AUC: 0.93691 | Weights = (0.0, 0.94, 0.06)\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000803 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 841\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "\n",
            "=== AUC (Train vs Valid) ===\n",
            "XGBoost     : 0.954100  |  0.933864\n",
            "LightGBM    : 0.970192  |  0.932045\n",
            "ExtraTrees  : 0.995139  |  0.928849\n",
            "Blend=1/3   : 0.982267  |  0.933885\n",
            "Blend*opt   : 0.974122  |  0.932507\n",
            "\n",
            "Best iters (CV):\n",
            "  XGB : [19, 202, 101, 1, 10, 207, 66, 462, 24, 33, 16, 16, 173, 30, 27, 309, 24, 91, 26, 421, 50, 15, 49, 41, 271, 73, 10, 8, 1017, 392, 50, 75, 86, 1698, 687, 89, 0, 1, 3, 15, 111, 248, 18, 11, 6, 5, 87, 4, 1, 48, 3, 8, 19, 507, 6, 15, 162, 7, 26, 25, 10, 173, 29, 465, 96, 989, 7, 15, 29, 3, 90, 61, 233, 111, 8, 414, 10, 28, 20, 17, 25, 125, 501, 1, 16, 10, 721, 1716, 5, 322, 1, 6, 1, 0, 19, 356, 12, 371, 1, 8, 58, 10, 403, 8, 27, 36, 557, 8, 15, 5, 320, 38, 523, 181, 89, 7, 337, 14, 0, 275, 223, 4, 2, 7, 3, 3, 0, 14, 90, 38, 1, 2, 8, 1, 15, 18, 49, 24, 80, 23, 3, 304, 23, 282, 31, 280, 1, 1, 3, 3]\n",
            "  LGBM: [56, 142, 420, 306, 262, 153, 115, 263, 198, 170, 204, 200, 237, 317, 247, 461, 118, 227, 586, 354, 642, 95, 61, 159, 1059, 199, 620, 288, 123, 742, 163, 181, 586, 429, 489, 190, 137, 127, 237, 259, 311, 536, 258, 215, 142, 111, 135, 215, 162, 317, 349, 118, 413, 1836, 201, 75, 304, 140, 195, 561, 957, 139, 512, 2053, 196, 125, 429, 497, 193, 660, 976, 242, 283, 108, 147, 655, 305, 154, 303, 212, 1566, 198, 799, 333, 496, 166, 2207, 198, 903, 992, 101, 431, 384, 436, 695, 427, 231, 2221, 210, 184, 76, 96, 519, 2064, 260, 192, 231, 209, 117, 680, 407, 302, 367, 163, 146, 142, 827, 531, 360, 482, 975, 237, 169, 317, 258, 254, 120, 337, 286, 303, 143, 318, 243, 624, 100, 621, 204, 396, 226, 184, 136, 351, 1221, 1259, 291, 546, 383, 273, 157, 494]\n"
          ]
        }
      ],
      "source": [
        "# ============================\n",
        "# Tabular Boosting: XGB (native)  LGBM  ExtraTrees\n",
        "# Early Stopping (XGB/LGBM), 10-Fold CV, OOF-weighted Blend\n",
        "# Prints train vs validation AUC per model and ensemble\n",
        "# ============================\n",
        "\n",
        "# !pip -q install --upgrade xgboost lightgbm > /dev/null\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.base import clone\n",
        "from sklearn.ensemble import ExtraTreesClassifier  # try\n",
        "\n",
        "import xgboost as xgb\n",
        "from lightgbm import LGBMClassifier\n",
        "import lightgbm as lgb\n",
        "\n",
        "#  SETTINGS\n",
        "RANDOM_STATE = 42\n",
        "N_SPLITS = 150  # 10 # 40\n",
        "EARLY_ROUNDS = 350  # 200 # 185\n",
        "\n",
        "DATA_PATH = \"train.csv\"     # set your path\n",
        "TARGET_COL = \"Exited\"       # target\n",
        "\n",
        "# Load data\n",
        "assert os.path.exists(DATA_PATH), f\"DATA_PATH not found: {DATA_PATH}\"\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "assert TARGET_COL in df.columns, f\"TARGET_COL '{TARGET_COL}' not in: {df.columns.tolist()}\"\n",
        "\n",
        "# Drop obvious IDs\n",
        "DROP_COLS = [c for c in df.columns if c.lower() in {\"id\", \"rowid\", \"customerid\", \"customer_id\"}]\n",
        "if DROP_COLS:\n",
        "    df = df.drop(columns=DROP_COLS)\n",
        "\n",
        "y = df[TARGET_COL].values\n",
        "X = df.drop(columns=[TARGET_COL])\n",
        "\n",
        "# drop surname\n",
        "if \"Surname\" in X.columns:\n",
        "    X = X.drop(columns=[\"Surname\"])\n",
        "\n",
        "# Types\n",
        "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "print(f\"Rows: {len(df):,} | Features: {X.shape[1]} | Num: {len(num_cols)} | Cat: {len(cat_cols)}\")\n",
        "\n",
        "# Split\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# LGBM/ET preprocessor: Ordinal on categories\n",
        "ordinal = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
        "pre_lgbm = ColumnTransformer(\n",
        "    transformers=[(\"cats\", ordinal, cat_cols)],\n",
        "    remainder=\"passthrough\"\n",
        ")\n",
        "\n",
        "#  Helpers\n",
        "def encode_for_xgb(X_fit: pd.DataFrame, X_a: pd.DataFrame, X_b: pd.DataFrame, cat_cols):\n",
        "    \"\"\"Fit an OrdinalEncoder on X_fit[cat_cols]; transform X_a/X_b to plain float32 NumPy arrays.\"\"\"\n",
        "    enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
        "    Xa = X_a.copy()\n",
        "    Xb = X_b.copy()\n",
        "    if cat_cols:\n",
        "        enc.fit(X_fit[cat_cols])\n",
        "        Xa[cat_cols] = enc.transform(Xa[cat_cols])\n",
        "        Xb[cat_cols] = enc.transform(Xb[cat_cols])\n",
        "    Xa = Xa.astype(np.float32).to_numpy(copy=False)\n",
        "    Xb = Xb.astype(np.float32).to_numpy(copy=False)\n",
        "    return Xa, Xb\n",
        "\n",
        "def prep_fit_transform(preprocessor: ColumnTransformer, X_tr: pd.DataFrame, X_va: pd.DataFrame):\n",
        "    \"\"\"Clones and fits the preprocessor on **X_tr**, then transforms **X_tr**/**X_va** to float32 numpy\"\"\"\n",
        "    prep = clone(preprocessor)\n",
        "    Xtr_enc = prep.fit_transform(X_tr)\n",
        "    Xva_enc = prep.transform(X_va)\n",
        "    if hasattr(Xtr_enc, \"toarray\"):\n",
        "        Xtr_enc = Xtr_enc.toarray()\n",
        "        Xva_enc = Xva_enc.toarray()\n",
        "    return Xtr_enc.astype(np.float32), Xva_enc.astype(np.float32), prep\n",
        "\n",
        "def xgb_predict_best(booster, dmatrix):\n",
        "    \"\"\"Compatible with all versions of XGBoost prediction on best iteration\"\"\"\n",
        "    import numpy as _np\n",
        "    bi = getattr(booster, \"best_iteration\", None)\n",
        "    if isinstance(bi, (int, _np.integer)) and bi is not None:\n",
        "        try:\n",
        "            return booster.predict(dmatrix, iteration_range=(0, int(bi) + 1))\n",
        "        except TypeError:\n",
        "            pass\n",
        "        try:\n",
        "            return booster.predict(dmatrix, ntree_limit=int(bi) + 1)\n",
        "        except TypeError:\n",
        "            pass\n",
        "    bntl = getattr(booster, \"best_ntree_limit\", None)\n",
        "    if isinstance(bntl, (int, _np.integer)) and bntl:\n",
        "        try:\n",
        "            return booster.predict(dmatrix, ntree_limit=int(bntl))\n",
        "        except TypeError:\n",
        "            pass\n",
        "    return booster.predict(dmatrix)\n",
        "\n",
        "#  CV (XGB native, LGBM numeric arrays, ExtraTrees numeric arrays)\n",
        "def cv_auc_model_es_xgb_lgbm_et(X, y, n_splits=5, early_rounds=200):\n",
        "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "    oof_xgb = np.zeros(len(y), dtype=float)\n",
        "    oof_lgb = np.zeros(len(y), dtype=float)\n",
        "    oof_et  = np.zeros(len(y), dtype=float)\n",
        "\n",
        "    iters_xgb, iters_lgb = [], []\n",
        "    folds_xgb, folds_lgb, folds_et = [], [], []\n",
        "\n",
        "    for fold, (trn_idx, val_idx) in enumerate(kf.split(X, y), 1):\n",
        "        X_tr, X_va = X.iloc[trn_idx], X.iloc[val_idx]\n",
        "        y_tr, y_va = y[trn_idx], y[val_idx]\n",
        "\n",
        "        #  XGBoost (native)\n",
        "        Xtr_enc, Xva_enc = encode_for_xgb(X_tr, X_tr, X_va, cat_cols)\n",
        "        dtr, dva = xgb.DMatrix(Xtr_enc, label=y_tr), xgb.DMatrix(Xva_enc, label=y_va)\n",
        "\n",
        "        params_xgb = {\n",
        "            \"objective\": \"binary:logistic\",\n",
        "            \"eval_metric\": \"auc\",\n",
        "            \"eta\": 0.02,\n",
        "            \"max_depth\": 8, #6\n",
        "            \"subsample\": 0.85,\n",
        "            \"colsample_bytree\": 0.85,\n",
        "            \"lambda\": 1.0,\n",
        "            \"alpha\": 0.0,\n",
        "            \"min_child_weight\": 6.0,\n",
        "            \"tree_method\": \"hist\",\n",
        "            \"seed\": RANDOM_STATE,\n",
        "        }\n",
        "        pos = int((y_tr == 1).sum()); neg = int((y_tr == 0).sum())\n",
        "        if pos > 0:\n",
        "            params_xgb[\"scale_pos_weight\"] = max(1.0, neg / pos)\n",
        "\n",
        "        bst = xgb.train(\n",
        "            params_xgb, dtr,\n",
        "            num_boost_round=10000,\n",
        "            evals=[(dva, \"valid\")],\n",
        "            early_stopping_rounds=early_rounds,\n",
        "            verbose_eval=False\n",
        "        )\n",
        "        pred_xgb = xgb_predict_best(bst, dva)\n",
        "        oof_xgb[val_idx] = pred_xgb\n",
        "        auc_x = roc_auc_score(y_va, pred_xgb)\n",
        "        folds_xgb.append(auc_x)\n",
        "        iters_xgb.append(getattr(bst, \"best_iteration\", None))\n",
        "\n",
        "        # ==== LightGBM preprocess ====\n",
        "        Xtr_lgb, Xva_lgb, _prep_lgb = prep_fit_transform(pre_lgbm, X_tr, X_va)\n",
        "        lgbm = LGBMClassifier(\n",
        "            n_estimators=5000, learning_rate=0.02,\n",
        "            num_leaves=63, min_child_samples=25,\n",
        "            subsample=0.9, colsample_bytree=0.85,\n",
        "            reg_alpha=0.0, reg_lambda=0.5,\n",
        "            objective=\"binary\", random_state=RANDOM_STATE\n",
        "        )\n",
        "        try:\n",
        "            lgbm.fit(\n",
        "                Xtr_lgb, y_tr,\n",
        "                eval_set=[(Xva_lgb, y_va)],\n",
        "                callbacks=[lgb.early_stopping(early_rounds, verbose=False)],\n",
        "            )\n",
        "        except TypeError:\n",
        "            lgbm.fit(\n",
        "                Xtr_lgb, y_tr,\n",
        "                eval_set=[(Xva_lgb, y_va)],\n",
        "                early_stopping_rounds=early_rounds,\n",
        "            )\n",
        "        pred_lgb = lgbm.predict_proba(Xva_lgb)[:, 1]\n",
        "        oof_lgb[val_idx] = pred_lgb\n",
        "        auc_l = roc_auc_score(y_va, pred_lgb)\n",
        "        folds_lgb.append(auc_l)\n",
        "        iters_lgb.append(getattr(lgbm, \"best_iteration_\", None))\n",
        "\n",
        "        #  ExtraTrees (preprocess)\n",
        "        Xtr_et, Xva_et, _prep_et = prep_fit_transform(pre_lgbm, X_tr, X_va)\n",
        "        et = ExtraTreesClassifier(\n",
        "            n_estimators=500,\n",
        "            max_depth=None,              # or 816 if overfitting\n",
        "            min_samples_leaf=2,\n",
        "            max_features=\"sqrt\",\n",
        "            bootstrap=False,\n",
        "            random_state=RANDOM_STATE,\n",
        "            n_jobs=-1,\n",
        "            class_weight=\"balanced\" if y_tr.mean() not in (0,1) else None\n",
        "        )\n",
        "        et.fit(Xtr_et, y_tr)\n",
        "        pred_et = et.predict_proba(Xva_et)[:, 1]\n",
        "        oof_et[val_idx] = pred_et\n",
        "        auc_e = roc_auc_score(y_va, pred_et)\n",
        "        folds_et.append(auc_e)\n",
        "\n",
        "        print(f\"Fold {fold}:  XGB={auc_x:.5f}  LGBM={auc_l:.5f}  ET={auc_e:.5f}\")\n",
        "\n",
        "    print(f\"\\nXGB  OOF AUC: {roc_auc_score(y, oof_xgb):.5f} | Folds: {[round(a,5) for a in folds_xgb]}\")\n",
        "    print(f\"LGBM OOF AUC: {roc_auc_score(y, oof_lgb):.5f} | Folds: {[round(a,5) for a in folds_lgb]}\")\n",
        "    print(f\"ET   OOF AUC: {roc_auc_score(y, oof_et ): .5f} | Folds: {[round(a,5) for a in folds_et ]}\")\n",
        "    return (oof_xgb, iters_xgb), (oof_lgb, iters_lgb), oof_et\n",
        "\n",
        "# ---- Run CV ----\n",
        "print(\"\\nTraining (ES, CV)...\")\n",
        "(xgb_oof, xgb_iters), (lgbm_oof, lgbm_iters), et_oof = cv_auc_model_es_xgb_lgbm_et(\n",
        "    X_train, y_train, n_splits=N_SPLITS, early_rounds=EARLY_ROUNDS\n",
        ")\n",
        "\n",
        "# ---- Optimize blend weights on OOF (wx, wl, we >=0, sum=1, step=0.01) ----\n",
        "best_auc, best_w = -1.0, (1/3, 1/3, 1/3)\n",
        "for wx in np.linspace(0, 1, 101):\n",
        "    wl_max = 1 - wx\n",
        "    for wl in np.linspace(0, wl_max, int(wl_max*100)+1):\n",
        "        we = 1 - wx - wl\n",
        "        blend = wx*xgb_oof + wl*lgbm_oof + we*et_oof\n",
        "        auc = roc_auc_score(y_train, blend)\n",
        "        if auc > best_auc:\n",
        "            best_auc, best_w = auc, (wx, wl, we)\n",
        "print(f\"\\nBest OOF blend AUC: {best_auc:.5f} | Weights = {tuple(round(w,3) for w in best_w)}\")\n",
        "\n",
        "# ------------ Final fit (Train vs Valid) ------------\n",
        "\n",
        "# XGB final (native)\n",
        "Xtr_enc, Xva_enc = encode_for_xgb(X_train, X_train, X_valid, cat_cols)\n",
        "dtr, dva = xgb.DMatrix(Xtr_enc, label=y_train), xgb.DMatrix(Xva_enc, label=y_valid)\n",
        "params_final = {\n",
        "    \"objective\": \"binary:logistic\",\n",
        "    \"eval_metric\": \"auc\",\n",
        "    \"eta\": 0.01,\n",
        "    \"max_depth\": 7,\n",
        "    \"subsample\": 0.85,\n",
        "    \"colsample_bytree\": 0.85,\n",
        "    \"lambda\": 3.0,\n",
        "    \"alpha\": 0.0,\n",
        "    \"min_child_weight\": 2.0,\n",
        "    \"tree_method\": \"hist\",\n",
        "    \"seed\": RANDOM_STATE,\n",
        "}\n",
        "pos = int((y_train == 1).sum()); neg = int((y_train == 0).sum())\n",
        "if pos > 0:\n",
        "    params_final[\"scale_pos_weight\"] = max(1.0, neg / pos)\n",
        "\n",
        "bst = xgb.train(\n",
        "    params_final, dtr,\n",
        "    num_boost_round=4000,\n",
        "    evals=[(dva, \"valid\")],\n",
        "    early_stopping_rounds=EARLY_ROUNDS,\n",
        "    verbose_eval=False\n",
        ")\n",
        "p_xgb_valid = xgb_predict_best(bst, dva)\n",
        "p_xgb_train = xgb_predict_best(bst, dtr)\n",
        "auc_xgb_train = roc_auc_score(y_train, p_xgb_train)\n",
        "auc_xgb_valid = roc_auc_score(y_valid, p_xgb_valid)\n",
        "\n",
        "# LGBM final (preprocess)\n",
        "Xtr_lgb, Xva_lgb, prep_final = prep_fit_transform(pre_lgbm, X_train, X_valid)\n",
        "lgbm_final = LGBMClassifier(\n",
        "    n_estimators=5000, learning_rate=0.02,\n",
        "    num_leaves=63, min_child_samples=25,\n",
        "    subsample=0.9, colsample_bytree=0.85,\n",
        "    reg_alpha=0.0, reg_lambda=0.5,\n",
        "    objective=\"binary\", random_state=RANDOM_STATE\n",
        ")\n",
        "try:\n",
        "    lgbm_final.fit(\n",
        "        Xtr_lgb, y_train,\n",
        "        eval_set=[(Xva_lgb, y_valid)],\n",
        "        callbacks=[lgb.early_stopping(EARLY_ROUNDS, verbose=False)],\n",
        "    )\n",
        "except TypeError:\n",
        "    lgbm_final.fit(\n",
        "        Xtr_lgb, y_train,\n",
        "        eval_set=[(Xva_lgb, y_valid)],\n",
        "        early_stopping_rounds=EARLY_ROUNDS,\n",
        "    )\n",
        "p_lgbm_valid = lgbm_final.predict_proba(Xva_lgb)[:, 1]\n",
        "p_lgbm_train = lgbm_final.predict_proba(Xtr_lgb)[:, 1]\n",
        "auc_lgbm_train = roc_auc_score(y_train, p_lgbm_train)\n",
        "auc_lgbm_valid = roc_auc_score(y_valid, p_lgbm_valid)\n",
        "\n",
        "# ExtraTrees final (preprocess)\n",
        "Xtr_et, Xva_et, prep_et_final = prep_fit_transform(pre_lgbm, X_train, X_valid)\n",
        "et_final = ExtraTreesClassifier(\n",
        "    n_estimators=500,\n",
        "    max_depth=None,\n",
        "    min_samples_leaf=2,\n",
        "    max_features=\"sqrt\",\n",
        "    bootstrap=False,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1,\n",
        "    class_weight=\"balanced\" if y_train.mean() not in (0,1) else None\n",
        ")\n",
        "et_final.fit(Xtr_et, y_train)\n",
        "p_et_valid = et_final.predict_proba(Xva_et)[:, 1]\n",
        "p_et_train = et_final.predict_proba(Xtr_et)[:, 1]\n",
        "auc_et_train = roc_auc_score(y_train, p_et_train)\n",
        "auc_et_valid = roc_auc_score(y_valid, p_et_valid)\n",
        "\n",
        "# Blends 3 models\n",
        "w_xgb, w_lgbm, w_et = best_w\n",
        "p_blend_eq_train = (p_xgb_train + p_lgbm_train + p_et_train) / 3.0\n",
        "p_blend_eq_valid = (p_xgb_valid + p_lgbm_valid + p_et_valid) / 3.0\n",
        "p_blend_opt_train = w_xgb*p_xgb_train + w_lgbm*p_lgbm_train + w_et*p_et_train\n",
        "p_blend_opt_valid = w_xgb*p_xgb_valid + w_lgbm*p_lgbm_valid + w_et*p_et_valid\n",
        "\n",
        "auc_blend_eq_train = roc_auc_score(y_train, p_blend_eq_train)\n",
        "auc_blend_eq_valid = roc_auc_score(y_valid, p_blend_eq_valid)\n",
        "auc_blend_opt_train = roc_auc_score(y_train, p_blend_opt_train)\n",
        "auc_blend_opt_valid = roc_auc_score(y_valid, p_blend_opt_valid)\n",
        "\n",
        "print(\"\\n=== AUC (Train vs Valid) ===\")\n",
        "print(f\"XGBoost     : {auc_xgb_train:.6f}  |  {auc_xgb_valid:.6f}\")\n",
        "print(f\"LightGBM    : {auc_lgbm_train:.6f}  |  {auc_lgbm_valid:.6f}\")\n",
        "print(f\"ExtraTrees  : {auc_et_train:.6f}  |  {auc_et_valid:.6f}\")\n",
        "print(f\"Blend=1/3   : {auc_blend_eq_train:.6f}  |  {auc_blend_eq_valid:.6f}\")\n",
        "print(f\"Blend*opt   : {auc_blend_opt_train:.6f}  |  {auc_blend_opt_valid:.6f}\")\n",
        "\n",
        "print(\"\\nBest iters (CV):\")\n",
        "print(\"  XGB :\", xgb_iters)\n",
        "print(\"  LGBM:\", lgbm_iters)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3fbf709-c03d-4bcb-8398-9861ca6d8458",
      "metadata": {
        "id": "f3fbf709-c03d-4bcb-8398-9861ca6d8458",
        "outputId": "fa1bf9d8-6b35-445f-9b8a-bb20f8a13db3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: submission_xgb.csv\n",
            "Preview:\n",
            "      id    Exited\n",
            "0  15000  0.393180\n",
            "1  15001  0.198907\n",
            "2  15002  0.284997\n",
            "3  15003  0.710505\n",
            "4  15004  0.239940\n"
          ]
        }
      ],
      "source": [
        "# ============================\n",
        "# SUBMISSION\n",
        "# ============================\n",
        "\n",
        "SUB_CHOICE = \"xgb\"          # <- pick: \"xgb\" | \"lgbm\" | \"et\" | \"blend_eq\" | \"blend_opt\"\n",
        "TEST_PATH = \"test.csv\"\n",
        "SAMPLE_SUB_PATH = \"sample_submission.csv\"\n",
        "SUB_PATH = f\"submission_{SUB_CHOICE}.csv\"\n",
        "\n",
        "import os, numpy as np, pandas as pd\n",
        "\n",
        "assert os.path.exists(TEST_PATH), f\"TEST_PATH not found: {TEST_PATH}\"\n",
        "test_df = pd.read_csv(TEST_PATH).copy()\n",
        "\n",
        "# --- choose ID column if present ---\n",
        "id_candidates = [c for c in test_df.columns if c.lower() in\n",
        "                 {\"id\",\"rowid\",\"customerid\",\"customer_id\",\"passengerid\",\"rownumber\"}]\n",
        "id_col = id_candidates[0] if id_candidates else None\n",
        "ids = test_df[id_col] if id_col else pd.Series(range(len(test_df)), name=\"Id\")\n",
        "\n",
        "# --- build X_test exactly like X used in training ---\n",
        "X_test = test_df.copy()\n",
        "\n",
        "# drop the same noise/ID columns as in training\n",
        "if \"Surname\" in X_test.columns and \"Surname\" in X.columns:\n",
        "    X_test = X_test.drop(columns=[\"Surname\"])\n",
        "if 'DROP_COLS' in globals() and DROP_COLS:\n",
        "    for c in DROP_COLS:\n",
        "        if c in X_test.columns:\n",
        "            X_test = X_test.drop(columns=[c])\n",
        "if id_col in X_test.columns:\n",
        "    X_test = X_test.drop(columns=[id_col])\n",
        "\n",
        "# align columns to training features\n",
        "X_test = X_test[X.columns]\n",
        "\n",
        "preds_dict = {}\n",
        "\n",
        "#  XGBoost (native)\n",
        "if 'bst' in globals():\n",
        "    _, Xte_enc = encode_for_xgb(X_train, X_train, X_test, cat_cols)\n",
        "    dte = xgb.DMatrix(Xte_enc)\n",
        "    preds_dict['xgb'] = xgb_predict_best(bst, dte)\n",
        "\n",
        "#  LightGBM\n",
        "if 'lgbm_final' in globals():\n",
        "    try:\n",
        "        Xte_lgb = prep_final.transform(X_test)    # fitted preprocessor from final fit\n",
        "    except NameError:\n",
        "        # fallback: fit encoder on X_train now\n",
        "        _, Xte_lgb, prep_final = prep_fit_transform(pre_lgbm, X_train, X_test)\n",
        "    if hasattr(Xte_lgb, \"toarray\"):\n",
        "        Xte_lgb = Xte_lgb.toarray()\n",
        "    preds_dict['lgbm'] = lgbm_final.predict_proba(Xte_lgb)[:, 1]\n",
        "\n",
        "#  ExtraTrees\n",
        "if 'et_final' in globals():\n",
        "    try:\n",
        "        Xte_et = prep_et_final.transform(X_test)  # If you saved a separate preprocessor for ET.\n",
        "    except NameError:\n",
        "        ## Use the same ordinal preprocessor as for LGBM.\n",
        "        try:\n",
        "            Xte_et = prep_final.transform(X_test)\n",
        "        except NameError:\n",
        "            _, Xte_et, prep_et_final = prep_fit_transform(pre_lgbm, X_train, X_test)\n",
        "    if hasattr(Xte_et, \"toarray\"):\n",
        "        Xte_et = Xte_et.toarray()\n",
        "    preds_dict['et'] = et_final.predict_proba(Xte_et)[:, 1]\n",
        "\n",
        "#  Blends\n",
        "keys = set(preds_dict.keys())\n",
        "\n",
        "if {\"xgb\", \"lgbm\", \"et\"} <= keys:\n",
        "    # 3-model blend\n",
        "    preds_dict['blend_eq'] = (preds_dict['xgb'] + preds_dict['lgbm'] + preds_dict['et']) / 3.0\n",
        "    try:\n",
        "        # best_weights from the training block: three weights are expected.\n",
        "        w_xgb, w_lgbm, w_et = best_w\n",
        "    except Exception:\n",
        "        w_xgb, w_lgbm, w_et = (1/3, 1/3, 1/3)\n",
        "    preds_dict['blend_opt'] = w_xgb*preds_dict['xgb'] + w_lgbm*preds_dict['lgbm'] + w_et*preds_dict['et']\n",
        "\n",
        "elif {\"xgb\", \"lgbm\"} <= keys:\n",
        "    # 2-model blend (fallback, if ET is missing).\n",
        "    preds_dict['blend_eq'] = (preds_dict['xgb'] + preds_dict['lgbm']) / 2.0\n",
        "    try:\n",
        "        w_xgb, w_lgbm = best_w\n",
        "    except Exception:\n",
        "        w_xgb, w_lgbm = (0.5, 0.5)\n",
        "    preds_dict['blend_opt'] = w_xgb*preds_dict['xgb'] + w_lgbm*preds_dict['lgbm']\n",
        "\n",
        "assert SUB_CHOICE in preds_dict, f\"SUB_CHOICE='{SUB_CHOICE}' not available. Have: {list(preds_dict)}\"\n",
        "preds = preds_dict[SUB_CHOICE]\n",
        "\n",
        "#  build submission\n",
        "if os.path.exists(SAMPLE_SUB_PATH):\n",
        "    sub = pd.read_csv(SAMPLE_SUB_PATH)\n",
        "    if id_col and id_col in sub.columns:\n",
        "        pred_cols = [c for c in sub.columns if c != id_col]\n",
        "        pred_col = pred_cols[0] if pred_cols else sub.columns[-1]\n",
        "        sub[id_col] = ids.values\n",
        "        sub[pred_col] = preds\n",
        "    else:\n",
        "        # fallback: first column id, last column target\n",
        "        sub.iloc[:, 0] = ids.values\n",
        "        sub.iloc[:, -1] = preds\n",
        "else:\n",
        "    sub = pd.DataFrame({ids.name if ids.name else \"Id\": ids.values, \"prediction\": preds})\n",
        "\n",
        "sub.to_csv(SUB_PATH, index=False)\n",
        "print(f\"Saved: {SUB_PATH}\")\n",
        "print(\"Preview:\")\n",
        "print(sub.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec0a8371-80b4-4ec1-afa7-2a402490ad0d",
      "metadata": {
        "id": "ec0a8371-80b4-4ec1-afa7-2a402490ad0d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}